import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin
import re

# Ù‚Ø±Ø§Ø¡Ø© Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù
SELECTED_CATS_FILE = "selected_cats.txt"
OUTPUT_FILE = "all_araaaab.m3u"

# Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø­Ø³Ø¨ tvg-id Ø£Ùˆ Ø§Ù„Ø§Ø³Ù…
EXCLUDE_IDS = ["1515459"]
EXCLUDE_NAMES = ["Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø®Ø¯Ù…Ø©"]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/117.0.0.0 Safari/537.36"
}

# Ù‚Ø§Ù…ÙˆØ³ Ù„ØªØ®Ø²ÙŠÙ† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¨Ø§Ù‚Ø§Øª ÙˆØ£Ø³Ù…Ø§Ø¦Ù‡Ø§ Ù…Ù† Ø§Ù„Ù…Ù„Ù
SELECTED_CATEGORIES = {}

def load_selected_cats_from_html():
    """Ù‚Ø±Ø§Ø¡Ø© Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¨Ø§Ù‚Ø§Øª ÙˆØ£Ø³Ù…Ø§Ø¦Ù‡Ø§ Ù…Ù† Ù…Ù„Ù HTML"""
    if not os.path.exists(SELECTED_CATS_FILE):
        print(f"âš ï¸ Ø§Ù„Ù…Ù„Ù {SELECTED_CATS_FILE} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
        return False
    
    print(f"ğŸ“‹ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù {SELECTED_CATS_FILE}...")
    
    with open(SELECTED_CATS_FILE, "r", encoding="utf-8") as f:
        content = f.read()
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup Ù„ØªØ­Ù„ÙŠÙ„ HTML
    soup = BeautifulSoup(content, "html.parser")
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¨Ø§Ù‚Ø§Øª
    for a in soup.select("a.nav-pill"):
        href = a.get("href", "")
        if "?cat=" in href:
            cat_id = href.split("=")[-1].strip()
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø© ÙˆØªÙ†Ø¸ÙŠÙÙ‡
            cat_name = a.text.strip()
            # Ø¥Ø²Ø§Ù„Ø© |AR| âœª ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
            cat_name = cat_name.replace("|AR|", "").replace("âœª", "").strip()
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
            cat_name = ' '.join(cat_name.split())
            
            SELECTED_CATEGORIES[cat_id] = cat_name
            print(f"  âœ… {cat_id}: {cat_name}")
    
    print(f"ğŸ“Š ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(SELECTED_CATEGORIES)} Ø¨Ø§Ù‚Ø© Ù…Ù† Ø§Ù„Ù…Ù„Ù")
    return len(SELECTED_CATEGORIES) > 0

def extract_channels_from_cat(cat_id, cat_name):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ù…Ù† Ø¨Ø§Ù‚Ø© Ù…Ø­Ø¯Ø¯Ø©"""
    print(f"ğŸ” Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨Ø§Ù‚Ø© {cat_id}: {cat_name}")
    
    url = f"https://v5on.site/index.php?cat={cat_id}"
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¨Ø§Ù‚Ø© {cat_id}: {e}")
        return []
    
    soup = BeautifulSoup(resp.text, "html.parser")
    channels = []
    
    for a in soup.select("a.channel-card"):
        href = a.get("href", "")
        if "play.php?id=" not in href:
            continue
        
        ch_id = href.split("id=")[-1].strip()
        name_tag = a.select_one(".card-info h4")
        name = name_tag.text.strip() if name_tag else f"Channel {ch_id}"
        
        # Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„Ù‚Ù†Ø§Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù€ ID Ø£Ùˆ Ø§Ù„Ø§Ø³Ù… Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…
        if ch_id in EXCLUDE_IDS or name in EXCLUDE_NAMES:
            print(f"âš ï¸ ØªÙ… Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„Ù‚Ù†Ø§Ø©: {name} (ID: {ch_id})")
            continue
        
        logo_tag = a.select_one(".card-thumbnail img")
        logo = logo_tag["src"] if logo_tag else ""
        channel_url = urljoin("https://v5on.site/", href)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø© Ù„Ù„Ù‚Ù†Ø§Ø©
        channels.append((ch_id, name, logo, channel_url, cat_name))
    
    print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(channels)} Ù‚Ù†Ø§Ø© ÙÙŠ Ø§Ù„Ø¨Ø§Ù‚Ø© {cat_id}")
    return channels

def main():
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ù…Ù† Ù…Ù„Ù HTML
    if not load_selected_cats_from_html():
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø¨Ø§Ù‚Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù„Ù.")
        return
    
    print(f"ğŸ“‹ Ø³ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {len(SELECTED_CATEGORIES)} Ø¨Ø§Ù‚Ø©")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª
    all_channels = []
    for cat_id, cat_name in SELECTED_CATEGORIES.items():
        channels = extract_channels_from_cat(cat_id, cat_name)
        all_channels.extend(channels)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© (Ù†ÙØ³ Ø§Ù„Ù€ ID)
    unique_channels = {}
    for ch in all_channels:
        ch_id, name, logo, url, cat_name = ch
        if ch_id not in unique_channels:
            unique_channels[ch_id] = ch
    
    final_channels = list(unique_channels.values())
    
    if not final_channels:
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù‚Ù†Ø§Ø©.")
        return
    
    # ÙƒØªØ§Ø¨Ø© Ù…Ù„Ù M3U Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© group-title (Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø©)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for ch_id, name, logo, channel_url, cat_name in final_channels:
            # Ø¥Ø¶Ø§ÙØ© group-title Ø§Ù„Ø°ÙŠ ÙŠÙ…Ø«Ù„ Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø©
            f.write(f'#EXTINF:-1 tvg-id="{ch_id}" tvg-name="{name}" tvg-logo="{logo}" group-title="{cat_name}",{name}\n')
            f.write(channel_url + "\n")
    
    print(f"âœ” ØªÙ… Ø­ÙØ¸ {len(final_channels)} Ù‚Ù†Ø§Ø© ÙÙŠ {OUTPUT_FILE}")
    print(f"ğŸ“Š ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… {len(set([ch[4] for ch in final_channels]))} Ø¨Ø§Ù‚Ø© Ù…Ø®ØªÙ„ÙØ©")

    # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù„ÙƒÙ„ Ø¨Ø§Ù‚Ø©
    print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨Ø§Ù‚Ø§Øª:")
    cat_stats = {}
    for ch in final_channels:
        cat_name = ch[4]
        if cat_name not in cat_stats:
            cat_stats[cat_name] = 0
        cat_stats[cat_name] += 1
    
    for cat_name, count in sorted(cat_stats.items()):
        print(f"  {cat_name}: {count} Ù‚Ù†Ø§Ø©")

if __name__ == "__main__":
    main()
