import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin
import re

# Ù‚Ø±Ø§Ø¡Ø© Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù
SELECTED_CATS_FILE = "selected_cats.txt"
OUTPUT_FILE = "all_ennnng.m3u"

# Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø­Ø³Ø¨ tvg-id Ø£Ùˆ Ø§Ù„Ø§Ø³Ù…
EXCLUDE_IDS = ["1515459"]
EXCLUDE_NAMES = ["Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø®Ø¯Ù…Ø©"]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/117.0.0.0 Safari/537.36"
}

# Ù‚Ø§Ù…ÙˆØ³ Ù„ØªØ®Ø²ÙŠÙ† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¨Ø§Ù‚Ø§Øª ÙˆØ£Ø³Ù…Ø§Ø¦Ù‡Ø§
CATEGORIES = {}

def load_categories_from_file():
    """Ù‚Ø±Ø§Ø¡Ø© Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¨Ø§Ù‚Ø§Øª ÙˆØ£Ø³Ù…Ø§Ø¦Ù‡Ø§ Ù…Ù† Ù…Ù„Ù selected_cats1.txt"""
    if not os.path.exists(SELECTED_CATS_FILE):
        print(f"âš ï¸ Ø§Ù„Ù…Ù„Ù {SELECTED_CATS_FILE} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
        return False
    
    print(f"ğŸ“‹ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ù…Ù† {SELECTED_CATS_FILE}...")
    
    with open(SELECTED_CATS_FILE, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙŠ ØªØ¨Ø¯Ø£ Ø¨Ù€ #
            if not line or line.startswith("#"):
                continue
            
            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø³Ø·Ø±: Ø±Ù‚Ù…_Ø§Ù„Ø¨Ø§Ù‚Ø© # Ø§Ø³Ù…_Ø§Ù„Ø¨Ø§Ù‚Ø©
            if "#" in line:
                parts = line.split("#", 1)
                cat_id = parts[0].strip()
                cat_name = parts[1].strip()
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø©
                cat_name = cat_name.replace("|AR|", "").replace("âœª", "").strip()
                cat_name = re.sub(r'\s+', ' ', cat_name)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
                
                if cat_id.isdigit():
                    CATEGORIES[cat_id] = cat_name
                    print(f"  âœ… {cat_id}: {cat_name}")
    
    print(f"ğŸ“Š ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(CATEGORIES)} Ø¨Ø§Ù‚Ø©")
    return len(CATEGORIES) > 0

def extract_channels_from_cat(cat_id, cat_name):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ù…Ù† Ø¨Ø§Ù‚Ø© Ù…Ø­Ø¯Ø¯Ø©"""
    print(f"ğŸ” Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨Ø§Ù‚Ø© {cat_id}: {cat_name}")
    
    url = f"https://v5on.site/index.php?cat={cat_id}"
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¨Ø§Ù‚Ø© {cat_id}: {e}")
        return []
    
    soup = BeautifulSoup(resp.text, "html.parser")
    channels = []
    
    for a in soup.select("a.channel-card"):
        href = a.get("href", "")
        if "play.php?id=" not in href:
            continue
        
        ch_id = href.split("id=")[-1].strip()
        name_tag = a.select_one(".card-info h4")
        name = name_tag.text.strip() if name_tag else f"Channel {ch_id}"
        
        # Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„Ù‚Ù†Ø§Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù€ ID Ø£Ùˆ Ø§Ù„Ø§Ø³Ù… Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…
        if ch_id in EXCLUDE_IDS or name in EXCLUDE_NAMES:
            print(f"âš ï¸ ØªÙ… Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„Ù‚Ù†Ø§Ø©: {name} (ID: {ch_id})")
            continue
        
        logo_tag = a.select_one(".card-thumbnail img")
        logo = logo_tag["src"] if logo_tag else ""
        if logo and not logo.startswith("http"):
            logo = urljoin("https://v5on.site/", logo)
        
        channel_url = urljoin("https://v5on.site/", href)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ø¹ Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø©
        channels.append({
            'id': ch_id,
            'name': name,
            'logo': logo,
            'url': channel_url,
            'category': cat_name  # Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
        })
    
    print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(channels)} Ù‚Ù†Ø§Ø©")
    return channels

def main():
    # 1. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù
    if not load_categories_from_file():
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø¨Ø§Ù‚Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù„Ù.")
        return
    
    # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ù…Ù† ÙƒÙ„ Ø¨Ø§Ù‚Ø©
    all_channels = []
    for cat_id, cat_name in CATEGORIES.items():
        channels = extract_channels_from_cat(cat_id, cat_name)
        all_channels.extend(channels)
    
    # 3. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© (Ù†ÙØ³ Ø§Ù„Ù€ ID)
    unique_channels = {}
    for ch in all_channels:
        if ch['id'] not in unique_channels:
            unique_channels[ch['id']] = ch
    
    final_channels = list(unique_channels.values())
    
    if not final_channels:
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù‚Ù†Ø§Ø©.")
        return
    
    # 4. ÙƒØªØ§Ø¨Ø© Ù…Ù„Ù M3U Ù…Ø¹ Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø© ÙÙŠ group-title
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        
        for ch in final_channels:
            # ÙƒØªØ§Ø¨Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ø¹ group-title = Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
            f.write(f'#EXTINF:-1 tvg-id="{ch["id"]}" '
                   f'tvg-name="{ch["name"]}" '
                   f'tvg-logo="{ch["logo"]}" '
                   f'group-title="{ch["category"]}",{ch["name"]}\n')
            f.write(ch["url"] + "\n")
    
    print(f"âœ” ØªÙ… Ø­ÙØ¸ {len(final_channels)} Ù‚Ù†Ø§Ø© ÙÙŠ {OUTPUT_FILE}")
    
    # 5. Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨Ø§Ù‚Ø§Øª
    print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨Ø§Ù‚Ø§Øª:")
    cat_stats = {}
    for ch in final_channels:
        cat_name = ch['category']
        cat_stats[cat_name] = cat_stats.get(cat_name, 0) + 1
    
    for cat_name, count in sorted(cat_stats.items()):
        print(f"  {cat_name}: {count} Ù‚Ù†Ø§Ø©")

if __name__ == "__main__":
    main()
